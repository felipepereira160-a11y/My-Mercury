import streamlit as st
import google.generativeai as genai
import pandas as pd

st.set_page_config(page_title="Seu Analista de Dados com IA", page_icon="üìä", layout="wide")

st.title("üìä Seu Analista de Dados com IA")
st.write("Converse comigo ou fa√ßa o upload de um arquivo na barra lateral para come√ßar a analisar!")

try:
    genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
except Exception as e:
    st.error("Chave de API do Google n√£o configurada.")
    st.stop()

# --- AQUI EST√Å A CORRE√á√ÉO ---
# Definimos o modelo aqui no in√≠cio, antes de qualquer parte que possa us√°-lo.
model = genai.GenerativeModel('gemini-pro-latest')

with st.sidebar:
    st.header("Adicionar Conhecimento")
    uploaded_file = st.sidebar.file_uploader("Fa√ßa o upload de um arquivo CSV ou XLSX", type=["csv", "xlsx"])
    if 'dataframe' not in st.session_state:
        st.session_state.dataframe = None
    if uploaded_file is not None:
        try:
            if uploaded_file.name.endswith('.csv'):
                df = pd.read_csv(uploaded_file, encoding='latin-1', sep=';', on_bad_lines='skip')
            elif uploaded_file.name.endswith('.xlsx'):
                df = pd.read_excel(uploaded_file, engine='openpyxl')
            st.session_state.dataframe = df
            st.success("Arquivo carregado!")
        except Exception as e:
            st.error(f"Erro ao ler o arquivo: {e}")
            
    if st.button("Limpar Arquivo e Chat"):
        st.session_state.dataframe = None
        # Agora o 'model' j√° existe e esta linha funciona.
        st.session_state.chat = model.start_chat(history=[])
        st.rerun()

if "chat" not in st.session_state:
    st.session_state.chat = model.start_chat(history=[])

if st.session_state.dataframe is not None:
    df = st.session_state.dataframe
    st.header("Dashboard do Arquivo")
    col1, col2, col3 = st.columns(3)
    col1.metric("Total de Linhas", f"{df.shape[0]:,}".replace(",", "."), "linhas")
    col2.metric("Total de Colunas", f"{df.shape[1]}", "colunas")
    coluna_cliente = next((col for col in df.columns if 'cliente' in col.lower()), None)
    if coluna_cliente:
        clientes_unicos = df[coluna_cliente].nunique()
        col3.metric("Clientes √önicos", f"{clientes_unicos}", "clientes")
    with st.expander("Ver pr√©-visualiza√ß√£o dos dados"):
        st.dataframe(df)
    st.header("Converse com seus Dados")

for message in st.session_state.chat.history:
    role = "assistant" if message.role == 'model' else 'user'
    with st.chat_message(role):
        st.markdown(message.parts[0].text)

def executar_analise_pandas(df, pergunta):
    prompt_engenharia = f"""
    Sua tarefa √© converter uma pergunta em uma √∫nica linha de c√≥digo Pandas que a responda.
    O dataframe est√° na vari√°vel `df`.
    Primeiras linhas do dataframe: {df.head().to_string()}
    REGRAS OBRIGAT√ìRIAS:
    1. A coluna 'Status' cont√©m: 'Agendada', 'Realizada', 'Nao Realizada', 'Reagendamento'.
    2. Se a pergunta contiver "agendadas", "realizadas", etc., voc√™ DEVE filtrar o dataframe por essa coluna ANTES de qualquer outra opera√ß√£o.
    Pergunta do usu√°rio: "{pergunta}"
    Baseado nas REGRAS, gere apenas a linha de c√≥digo Pandas necess√°ria.
    """
    try:
        code_response = genai.GenerativeModel('gemini-pro-latest').generate_content(prompt_engenharia)
        codigo_pandas = code_response.text.strip().replace('`', '').replace('python', '').strip()
        resultado = eval(codigo_pandas, {'df': df, 'pd': pd})
        return resultado, None
    except Exception as e:
        return None, f"Ocorreu um erro ao executar a an√°lise: {e}"

if prompt := st.chat_input("Converse com a IA ou fa√ßa uma pergunta sobre seus dados..."):
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.chat.history.append({'role': 'user', 'parts': [{'text': prompt}]})

    if st.session_state.dataframe is not None:
        response_container = st.chat_message("assistant")
        with response_container:
            st.markdown("Analisando os dados...")
            resultado_analise, erro = executar_analise_pandas(st.session_state.dataframe, prompt)
        
            if erro:
                st.error(erro)
                response_text = "Desculpe, n√£o consegui analisar os dados. Tente uma pergunta mais simples."
            else:
                if isinstance(resultado_analise, (pd.Series, pd.DataFrame)) and len(resultado_analise) > 1:
                    with response_container:
                        st.write("Aqui est√° uma visualiza√ß√£o para sua pergunta:")
                        st.bar_chart(resultado_analise)
                        try:
                            total_items = len(resultado_analise)
                            top_item_name = resultado_analise.index[0]
                            top_item_value = resultado_analise.iloc[0]
                            contexto_para_ia = f"O gr√°fico mostra um total de {total_items} itens. O item com o maior valor √© '{top_item_name}' com {top_item_value}."
                        except Exception:
                            contexto_para_ia = "Um gr√°fico foi gerado."
                        prompt_final = f"""A pergunta foi: "{prompt}". Um gr√°fico j√° foi exibido. Com base no resumo dos dados a seguir, escreva uma breve an√°lise amig√°vel: {contexto_para_ia}"""
                else:
                    prompt_final = f"""A pergunta foi: "{prompt}". O resultado da an√°lise foi: {resultado_analise}. Formule uma resposta amig√°vel e direta."""
                
                response = genai.GenerativeModel('gemini-pro-latest').generate_content(prompt_final)
                response_text = response.text
                response_container.markdown(response_text)
    else:
        response = st.session_state.chat.send_message(prompt)
        response_text = response.text
        with st.chat_message("assistant"):
            st.markdown(response_text)

    st.session_state.chat.history.append({'role': 'model', 'parts': [{'text': response_text}]})
